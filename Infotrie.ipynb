{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of EUR/USD FX with sentiment data\n",
    "\n",
    "In this notebook we applied some simple machine learning technique in order to try to predict the EUR-USD market price.\n",
    "\n",
    "We have at our disposal chronological data spanning from 18th of March 2016 to 23rd of September 2016 for both price and sentiment data. We also have at our disposal some linear and non linear transformations of these data, in particular simple moving averages and exponential moving averages.\n",
    "\n",
    "The model we assume here is that market price at time t follows a Normal distribution of which the mean is a linear combination of the historical features mentionned above and the variance is constant.\n",
    "\n",
    "```\n",
    "p[t] ~ N(intercept + alpha_1 * feature_1 + ... + alpha_k * feature_k, sigma)\n",
    "```\n",
    "\n",
    "We try to find the `intercept` and the `alpha_i` by maximizing the Likelihood of those paraemters on the training set we will select, which, all calculations made, boils down to minimizing the sum of squared error between observations and estimate : `|p_1[t] - p*_1[t]|^2 + ... + |p_N[t] - p*_N[t]|^2`. We assess the quality of our solution by computing the `r2` parameter on a validation set, which is distinct from the training set (actually it corresponds to a period following the training set). The `r2` parameter captures the part of the variance that is explained by the model.\n",
    "\n",
    "Such optimal set of coefficients is unique if the covariance matrix of the features is non singular, which means that features are linearly independent. However in the present case many features are either equal (we drop these to start with) or quite similar, leading to poor generalization results on the testing set. Thus we perform some feature selection. In the present case we picked random feature bags, as the small size of the data set lets us perform trainings quite quickly. Another approach would have been to use the Lasso regularization (L1 norm) which favors sparse solutions : less meaningful features coefficient are set to 0.\n",
    "\n",
    "We have used 3 variations of the linear regression :\n",
    "- Ordinary least squared : which is classical linear regression\n",
    "- Ridge regression : which penalizes models that have to high coefficient to limit overfitting and yield to better results on the validation set (this method needs one extra hyper-parameter to be defined, to control the regularization power of the model, named `alpha` in the code). From a bayesian stand point this boils down to making the assumptions that the prior distribution of the coefficients follow a Normal law of mean 0 and variance `sqrt(alpha)`\n",
    "- Bayesian regression : which makes the assumption that the prior distribution of the coefficient follows a log-normal law, and optimizes the slightly modified resulting likelihood.\n",
    "\n",
    "## Results\n",
    "\n",
    "We have carried out 31 trainings on 20 week long training sets and validated on the following 2 weeks. There is an offset of 1 day between each pair of training/testing sets.\n",
    "\n",
    "The use of sentimental data combined with historical price data yields to good results compared to basic models that resorts only on historical price data. At the end of this notebooks models are ranked according their average performance (average r2) on all the 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import functools\n",
    "import dateparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Data Set\n",
    "parse_fn = lambda s: dateparser.parse(date_string = s, date_formats = [\"%d-%m-%Y\"])\n",
    "df = pd.read_csv(\"./data/EUR_USD.csv\", parse_dates=[0], date_parser=parse_fn, infer_datetime_format = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing pHigh(same as pOpen)\n",
      "removing pLow(same as pOpen)\n",
      "removing pClose(same as pOpen)\n",
      "removing sOpen(same as sentiment)\n",
      "removing sClose(same as sentiment)\n",
      "removing SumSentiment(same as sentiment)\n",
      "removing AvgSentiment(same as sentiment)\n"
     ]
    }
   ],
   "source": [
    "# Some columns look the same...\n",
    "identical_columns = {\n",
    "    'pOpen' : ['pHigh', 'pLow', 'pClose'],\n",
    "    'sentiment' : ['sHigh', 'sLow', 'sOpen', 'sClose', 'sBuzz', 'SumSentiment', 'AvgSentiment']\n",
    "}\n",
    "for ref, cols_to_check in identical_columns.items() :\n",
    "    it = zip(cols_to_check, map(lambda x: (df[ref] == df[x]).all(), cols_to_check))\n",
    "    for col, do_drop in it :\n",
    "        if do_drop :\n",
    "            print('removing ' + col + '(same as ' + ref + ')')\n",
    "            df = df.drop([col], axis = 1)\n",
    "\n",
    "# ... rename the only one left\n",
    "df = df.rename(columns = {'pOpen': 'p'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing pVolume (all values are 0)\n"
     ]
    }
   ],
   "source": [
    "# pVolume seems always worth 0\n",
    "if (df['pVolume'] == 0).all() :\n",
    "    print('removing pVolume (all values are 0)')\n",
    "    df = df.drop(['pVolume'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# price values seem to be missing for some rows, fill them with previous values\n",
    "pSma_cols = list(filter(lambda col: col.startswith('pSma'), df.columns))\n",
    "pSma_cols.insert(0, 'p')\n",
    "\n",
    "null_cols_to_concat = []\n",
    "for col in pSma_cols:\n",
    "    null_cols_to_concat.append(df[df[col] == 0.0][col])\n",
    "\n",
    "null_pSma_df = pd.concat(null_cols_to_concat, axis = 1)\n",
    "\n",
    "# if some NaN in null_price_df, then some rows were mistmatching in each\n",
    "# individual selected columns. This means that values in these selected columns rows are not all null\n",
    "all_pSma_values_simultaneously_null = not null_pSma_df.isnull().values.any()\n",
    "\n",
    "df0 = df.copy()\n",
    "for col in pSma_cols:\n",
    "    df0[col].replace(to_replace=0, inplace='true', method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add historical features for k days\n",
    "k = 5\n",
    "historical_columns = {}\n",
    "feature_to_historize = ['p', 'sentiment']\n",
    "for feat in feature_to_historize :\n",
    "    for i in range(1, k) :\n",
    "        hist_feat = df0[feat].shift(i)\n",
    "        historical_columns[feat + '_minus_' + str(i)] = hist_feat\n",
    "\n",
    "historical_df = pd.DataFrame(historical_columns)\n",
    "df1 = pd.concat([df0, historical_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removing SMA_3 since k > 3 and SMA_3 are linear combination of historical features\n",
    "df1 = df1.drop(['pSma_3', 'sSma_3'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dummy encoding for the 'match' feature\n",
    "match_feature = df1['match'].apply(lambda boolean: float(boolean))\n",
    "nomatch_feature = df1['match'].apply(lambda boolean: float(not boolean))\n",
    "dummy_encoded_match_df = pd.DataFrame({'match': match_feature, 'nomatch': nomatch_feature})\n",
    "df1 = df1.drop(['match'], axis = 1)\n",
    "df1 = pd.concat([df1, dummy_encoded_match_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add p at day + 1 (which is the target variable we will want to predict)\n",
    "label_df = pd.DataFrame({'p_plus_1': df1['p'].shift(-1)})\n",
    "df1 = pd.concat([df1, label_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop edge rows where historical values are not defined\n",
    "df1 = df1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'p', 'pSma_w', 'pEma_w', 'pEma_3', 'pSma_7', 'pEma_7',\n",
       "       'pSma_15', 'pEma_15', 'pSma_10', 'pEma_10', 'pSma_30', 'pEma_30',\n",
       "       'pSma_60', 'pEma_60', 'pSma_90', 'pEma_90', 'pK', 'pD', 'pMacd',\n",
       "       'pSignal', 'sHigh', 'sLow', 'sBuzz', 'sVolume', 'sentiment', 'sSma_w',\n",
       "       'sEma_w', 'sEma_3', 'sSma_7', 'sEma_7', 'sSma_15', 'sEma_15', 'sSma_10',\n",
       "       'sEma_10', 'sSma_30', 'sEma_30', 'sSma_60', 'sEma_60', 'sSma_90',\n",
       "       'sEma_90', 'sK', 'sD', 'sMacd', 'sSignal', 'pMacd_Hist', 'sMacd_Hist',\n",
       "       'p_minus_1', 'p_minus_2', 'p_minus_3', 'p_minus_4', 'sentiment_minus_1',\n",
       "       'sentiment_minus_2', 'sentiment_minus_3', 'sentiment_minus_4', 'match',\n",
       "       'nomatch', 'p_plus_1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select consecutive training and validation sets\n",
    "train_set_size = 20 * 7 # 20 weeks\n",
    "test_set_size = 2 * 7   # 2 weeks\n",
    "sliding_offset = 1      # 1 day\n",
    "\n",
    "train_sets = []\n",
    "test_sets = []\n",
    "train_set_start = 0\n",
    "train_set_end = train_set_start + train_set_size\n",
    "test_set_start = train_set_end\n",
    "test_set_end = test_set_start + test_set_size\n",
    "\n",
    "while test_set_end < len(df1.index):\n",
    "    train_sets.append(df1[train_set_start:train_set_end])\n",
    "    test_sets.append(df1[test_set_start:test_set_end])\n",
    "    train_set_start += sliding_offset\n",
    "    train_set_end += sliding_offset\n",
    "    test_set_start += sliding_offset\n",
    "    test_set_end += sliding_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up 20 random bags of 10 features\n",
    "available_features = df1.drop(['date', 'p_plus_1'], axis = 1).columns.values\n",
    "\n",
    "random_feature_bags = {}\n",
    "for i in range(20) :\n",
    "    copy = available_features.copy()\n",
    "    np.random.shuffle(copy)\n",
    "    random_feature_bags['bag' + str(i)] = copy[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag0 :\n",
      "['pEma_90' 'sentiment_minus_3' 'sLow' 'p_minus_1' 'pEma_3' 'sSma_60'\n",
      " 'sEma_3' 'sSma_30' 'pK' 'pSma_60']\n",
      "bag1 :\n",
      "['nomatch' 'pSma_7' 'sSma_w' 'pD' 'sEma_w' 'pEma_15' 'p_minus_3' 'pEma_3'\n",
      " 'p' 'sHigh']\n",
      "bag10 :\n",
      "['p_minus_4' 'sD' 'pSma_15' 'pEma_7' 'pMacd_Hist' 'sentiment' 'sSma_w'\n",
      " 'pEma_w' 'sEma_w' 'sentiment_minus_1']\n",
      "bag11 :\n",
      "['pEma_7' 'p' 'sSma_60' 'pMacd' 'sSma_7' 'sSma_10' 'sentiment_minus_3'\n",
      " 'sEma_7' 'pSma_w' 'pSma_60']\n",
      "bag12 :\n",
      "['sSma_90' 'pSma_7' 'pEma_3' 'sEma_7' 'pK' 'sEma_3' 'pSignal' 'sBuzz'\n",
      " 'match' 'sSma_30']\n",
      "bag13 :\n",
      "['sHigh' 'sEma_90' 'sentiment_minus_1' 'pEma_90' 'sSignal' 'p_minus_2'\n",
      " 'sVolume' 'sEma_30' 'sEma_7' 'sSma_30']\n",
      "bag14 :\n",
      "['sMacd_Hist' 'sEma_90' 'pEma_30' 'pEma_7' 'sVolume' 'pSma_w'\n",
      " 'sentiment_minus_2' 'sSma_90' 'sEma_3' 'sentiment_minus_3']\n",
      "bag15 :\n",
      "['nomatch' 'pSma_w' 'sentiment' 'sMacd_Hist' 'pSignal' 'sEma_10' 'pSma_7'\n",
      " 'p_minus_3' 'sVolume' 'sSma_15']\n",
      "bag16 :\n",
      "['sEma_15' 'sentiment_minus_1' 'sentiment_minus_2' 'pSma_60' 'p_minus_1'\n",
      " 'sD' 'p' 'pEma_10' 'match' 'sLow']\n",
      "bag17 :\n",
      "['sMacd_Hist' 'match' 'pSignal' 'pSma_60' 'pSma_10' 'nomatch' 'pEma_3'\n",
      " 'pEma_w' 'pEma_30' 'pEma_15']\n",
      "bag18 :\n",
      "['pD' 'pSma_10' 'pEma_3' 'pEma_10' 'sEma_30' 'sentiment_minus_1'\n",
      " 'p_minus_3' 'pEma_90' 'pMacd_Hist' 'p_minus_2']\n",
      "bag19 :\n",
      "['pSma_15' 'pSignal' 'p_minus_3' 'p_minus_4' 'nomatch' 'p_minus_2'\n",
      " 'sSma_60' 'pEma_3' 'sSma_90' 'p']\n",
      "bag2 :\n",
      "['pSma_90' 'pMacd_Hist' 'sSma_60' 'pSignal' 'sEma_90' 'pK' 'p_minus_2'\n",
      " 'sentiment' 'nomatch' 'sentiment_minus_1']\n",
      "bag3 :\n",
      "['sSma_w' 'sD' 'pMacd' 'pSma_7' 'p_minus_1' 'match' 'sentiment_minus_1'\n",
      " 'pEma_w' 'pEma_15' 'sSma_15']\n",
      "bag4 :\n",
      "['p_minus_2' 'sSma_7' 'sSma_15' 'sEma_10' 'nomatch' 'sEma_w' 'pEma_15' 'pK'\n",
      " 'sMacd' 'match']\n",
      "bag5 :\n",
      "['pSma_90' 'sEma_30' 'sEma_15' 'pEma_7' 'sEma_60' 'sSma_7' 'sEma_7'\n",
      " 'sEma_10' 'sD' 'pEma_3']\n",
      "bag6 :\n",
      "['sEma_w' 'sMacd' 'match' 'pEma_90' 'sSma_15' 'sEma_30' 'pEma_3' 'sEma_10'\n",
      " 'pSignal' 'sEma_60']\n",
      "bag7 :\n",
      "['pSma_30' 'p' 'sEma_3' 'sD' 'sSma_15' 'sentiment_minus_1' 'pEma_90'\n",
      " 'pEma_w' 'sEma_30' 'pSma_10']\n",
      "bag8 :\n",
      "['pK' 'sSma_7' 'sHigh' 'pEma_15' 'match' 'sEma_90' 'pSma_60' 'sEma_10'\n",
      " 'sSma_30' 'sLow']\n",
      "bag9 :\n",
      "['sEma_60' 'sEma_90' 'sMacd_Hist' 'sEma_30' 'nomatch' 'p_minus_4' 'sEma_7'\n",
      " 'pSma_60' 'sentiment_minus_2' 'sentiment']\n"
     ]
    }
   ],
   "source": [
    "for bag in sorted(random_feature_bags.keys()):\n",
    "    print(bag + ' :')\n",
    "    print(random_feature_bags[bag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test some basic bags of features as well along the way\n",
    "random_feature_bags['baseBag0'] = ['p']\n",
    "random_feature_bags['baseBag1'] = ['sentiment']\n",
    "random_feature_bags['baseBag2'] = ['p', 'p_minus_1']\n",
    "random_feature_bags['baseBag3'] = ['p', 'sentiment']\n",
    "random_feature_bags['baseBag4'] = ['p', 'p_minus_1', 'sentiment']\n",
    "random_feature_bags['baseBag5'] = ['p', 'p_minus_1', 'sBuzz', 'sentiment']\n",
    "random_feature_bags['baseBag6'] = ['p', 'p_minus_1', 'sBuzz', 'sentiment', 'sVolume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/scipy/linalg/basic.py:884: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  warnings.warn(mesg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# Perform feature selection for linear regression, bayesian linear regression, bayesian linear regression\n",
    "from sklearn import linear_model\n",
    "\n",
    "ols = linear_model.LinearRegression()\n",
    "bayesian=linear_model.BayesianRidge()\n",
    "ridge = linear_model.Ridge()\n",
    "\n",
    "res = {}\n",
    "\n",
    "for key, bag in random_feature_bags.items() :\n",
    "\n",
    "    ols_r2s = []\n",
    "    for i in range(len(train_sets)) :\n",
    "        train_df = train_sets[i].copy()\n",
    "        train_Y = train_df.as_matrix(['p_plus_1']).flatten()\n",
    "        train_X = (train_df[bag]).as_matrix()\n",
    "        ols.fit(train_X, train_Y)\n",
    "\n",
    "        test_df = test_sets[i].copy()\n",
    "        test_Y = test_df.as_matrix(['p_plus_1']).flatten()\n",
    "        test_X = (test_df[bag]).as_matrix()\n",
    "        r2 = ols.score(test_X, test_Y)\n",
    "\n",
    "        ols_r2s.append(r2)\n",
    "\n",
    "    res[key] = {}\n",
    "    res[key][\"ols\"] = {}\n",
    "#    res[key][\"ols\"][\"r2s\"] = ols_r2s\n",
    "    res[key][\"ols\"][\"avg_r2\"] = np.average(ols_r2s)\n",
    "\n",
    "    bayesian_r2s = []\n",
    "    for i in range(len(train_sets)) :\n",
    "        train_df = train_sets[i].copy()\n",
    "        train_Y = train_df.as_matrix(['p_plus_1']).flatten()\n",
    "        train_X = (train_df[bag]).as_matrix()\n",
    "        bayesian.fit(train_X, train_Y)\n",
    "\n",
    "        test_df = test_sets[i].copy()\n",
    "        test_Y = test_df.as_matrix(['p_plus_1']).flatten()\n",
    "        test_X = (test_df[bag]).as_matrix()\n",
    "        r2 = bayesian.score(test_X, test_Y)\n",
    "        bayesian_r2s.append(r2)\n",
    "\n",
    "    res[key][\"bayesian\"] = {}\n",
    "#    res[key][\"bayesian\"][\"r2s\"] = bayesian_r2s\n",
    "    res[key][\"bayesian\"][\"avg_r2\"] = np.average(bayesian_r2s)\n",
    "\n",
    "    alphas = [0.0001, 0.001, 0.01, 0.1, 1.0, 20.0, 100.0, 10000.0]\n",
    "    ridge_r2s = []\n",
    "    for a in alphas :\n",
    "        r2s = []\n",
    "        ridge.set_params(alpha = a)\n",
    "        for i in range(len(train_sets)) :\n",
    "            train_df = train_sets[i].copy()\n",
    "            train_Y = train_df.as_matrix(['p_plus_1']).flatten()\n",
    "            train_X = (train_df[bag]).as_matrix()\n",
    "            ridge.fit(train_X, train_Y)\n",
    "\n",
    "            test_df = test_sets[i].copy()\n",
    "            test_Y = test_df.as_matrix(['p_plus_1']).flatten()\n",
    "            test_X = (test_df[bag]).as_matrix()\n",
    "            r2 = ridge.score(test_X, test_Y)\n",
    "\n",
    "            r2s.append(r2)\n",
    "\n",
    "        ridge_r2s.append(r2s)\n",
    "\n",
    "    avg_r2s = list(map(lambda x: np.average(x), ridge_r2s))\n",
    "\n",
    "    best_alpha_index = 0\n",
    "    max_r2 = avg_r2s[best_alpha_index]\n",
    "    for i in range(len(avg_r2s)):\n",
    "        if avg_r2s[i] > max_r2 :\n",
    "            best_alpha_index = i\n",
    "            max_r2 = avg_r2s[best_alpha_index]\n",
    "\n",
    "    res[key][\"ridge\"] = {}\n",
    "#    res[key][\"ridge\"][\"r2s\"] = ridge_r2s[best_alpha_index]\n",
    "    res[key][\"ridge\"][\"avg_r2\"] = avg_r2s[best_alpha_index]\n",
    "    res[key][\"ridge\"][\"best_alpha\"] = alphas[best_alpha_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag0 :\n",
      "\tridge :\n",
      "\t{'avg_r2': 0.67677018221609264, 'best_alpha': 0.0001}\n",
      "\tbayesian :\n",
      "\t{'avg_r2': 0.67248447912815812}\n",
      "\tols :\n",
      "\t{'avg_r2': 0.65255133696556811}\n",
      "bag1 :\n",
      "\tridge :\n",
      "\t{'avg_r2': 0.64884838165243131, 'best_alpha': 0.0001}\n",
      "\tbayesian :\n",
      "\t{'avg_r2': 0.66119237582282697}\n",
      "\tols :\n",
      "\t{'avg_r2': 0.51069346712102037}\n",
      "bag10 :\n",
      "\tridge :\n",
      "\t{'avg_r2': 0.53767498498467015, 'best_alpha': 0.0001}\n",
      "\tbayesian :\n",
      "\t{'avg_r2': 0.69343128357595962}\n",
      "\tols :\n",
      "\t{'avg_r2': 0.68329942899647744}\n",
      "bag11 :\n",
      "\tridge :\n",
      "\t{'avg_r2': 0.6346855732570591, 'best_alpha': 0.001}\n",
      "\tbayesian :\n",
      "\t{'avg_r2': 0.62937572433744227}\n",
      "\tols :\n",
      "\t{'avg_r2': 0.57814072928922677}\n",
      "bag12 :\n",
      "\tridge :\n",
      "\t{'avg_r2': 0.71441742628924731, 'best_alpha': 0.001}\n",
      "\tbayesian :\n",
      "\t{'avg_r2': 0.67287537566139033}\n",
      "\tols :\n",
      "\t{'avg_r2': 0.61951756437377881}\n",
      "bag13 :\n",
      "\tridge :\n",
      "\t{'avg_r2': -0.66815322424987655, 'best_alpha': 0.1}\n",
      "\tbayesian :\n",
      "\t{'avg_r2': -1.0121726888995655}\n",
      "\tols :\n",
      "\t{'avg_r2': -1.7157402459543285}\n",
      "bag14 :\n",
      "\tridge :\n",
      "\t{'avg_r2': 0.51207574839296166, 'best_alpha': 0.001}\n",
      "\tbayesian :\n",
      "\t{'avg_r2': 0.47663922702880018}\n",
      "\tols :\n",
      "\t{'avg_r2': 0.34425915829167741}\n",
      "bag15 :\n",
      "\tridge :\n",
      "\t{'avg_r2': 0.51570874253648014, 'best_alpha': 0.0001}\n",
      "\tbayesian :\n",
      "\t{'avg_r2': 0.54223349213583572}\n",
      "\tols :\n",
      "\t{'avg_r2': 0.55443959736458526}\n",
      "bag16 :\n",
      "\tridge :\n",
      "\t{'avg_r2': 0.47965665693032966, 'best_alpha': 0.001}\n",
      "\tbayesian :\n",
      "\t{'avg_r2': 0.48286975835803597}\n",
      "\tols :\n",
      "\t{'avg_r2': 0.47405506922222962}\n",
      "bag17 :\n",
      "\tridge :\n",
      "\t{'avg_r2': 0.74249819521566895, 'best_alpha': 0.0001}\n",
      "\tbayesian :\n",
      "\t{'avg_r2': 0.6798560219184886}\n",
      "\tols :\n",
      "\t{'avg_r2': -0.11611668720794097}\n",
      "bag18 :\n",
      "\tridge :\n",
      "\t{'avg_r2': 0.69976434877674021, 'best_alpha': 0.0001}\n",
      "\tbayesian :\n",
      "\t{'avg_r2': 0.64362914591296871}\n",
      "\tols :\n",
      "\t{'avg_r2': 0.43435064366546633}\n",
      "bag19 :\n",
      "\tridge :\n",
      "\t{'avg_r2': 0.68541097919825733, 'best_alpha': 0.0001}\n",
      "\tbayesian :\n",
      "\t{'avg_r2': 0.68124896422556414}\n",
      "\tols :\n",
      "\t{'avg_r2': 0.6808791650091911}\n",
      "bag2 :\n",
      "\tridge :\n",
      "\t{'avg_r2': 0.073724568203792598, 'best_alpha': 0.0001}\n",
      "\tbayesian :\n",
      "\t{'avg_r2': -0.28167601480321597}\n",
      "\tols :\n",
      "\t{'avg_r2': -0.40043666278526152}\n",
      "bag3 :\n",
      "\tridge :\n",
      "\t{'avg_r2': 0.58259669906317435, 'best_alpha': 0.0001}\n",
      "\tbayesian :\n",
      "\t{'avg_r2': 0.5928477733441414}\n",
      "\tols :\n",
      "\t{'avg_r2': 0.5914140039556901}\n",
      "bag4 :\n",
      "\tridge :\n",
      "\t{'avg_r2': -0.22329599620651749, 'best_alpha': 0.01}\n",
      "\tbayesian :\n",
      "\t{'avg_r2': -0.45175395534676099}\n",
      "\tols :\n",
      "\t{'avg_r2': -0.57371344142776637}\n",
      "bag5 :\n",
      "\tridge :\n",
      "\t{'avg_r2': 0.67187070103622959, 'best_alpha': 0.0001}\n",
      "\tbayesian :\n",
      "\t{'avg_r2': 0.66865324124776548}\n",
      "\tols :\n",
      "\t{'avg_r2': 0.66648905773068534}\n",
      "bag6 :\n",
      "\tridge :\n",
      "\t{'avg_r2': 0.71562062096463464, 'best_alpha': 0.001}\n",
      "\tbayesian :\n",
      "\t{'avg_r2': 0.55609175570401248}\n",
      "\tols :\n",
      "\t{'avg_r2': 0.22152285552237524}\n",
      "bag7 :\n",
      "\tridge :\n",
      "\t{'avg_r2': 0.61583656544271537, 'best_alpha': 0.0001}\n",
      "\tbayesian :\n",
      "\t{'avg_r2': 0.6127221162013049}\n",
      "\tols :\n",
      "\t{'avg_r2': 0.57725402903618772}\n",
      "bag8 :\n",
      "\tridge :\n",
      "\t{'avg_r2': -0.18628147174484608, 'best_alpha': 0.001}\n",
      "\tbayesian :\n",
      "\t{'avg_r2': -0.16885171995546924}\n",
      "\tols :\n",
      "\t{'avg_r2': -0.37661042989068794}\n",
      "bag9 :\n",
      "\tridge :\n",
      "\t{'avg_r2': -1.4488452454060718, 'best_alpha': 0.1}\n",
      "\tbayesian :\n",
      "\t{'avg_r2': -2.4304029625304411}\n",
      "\tols :\n",
      "\t{'avg_r2': -2.9526867980422717}\n",
      "baseBag0 :\n",
      "\tridge :\n",
      "\t{'avg_r2': 0.38491775495880048, 'best_alpha': 0.0001}\n",
      "\tbayesian :\n",
      "\t{'avg_r2': 0.38538130934360143}\n",
      "\tols :\n",
      "\t{'avg_r2': 0.3855297494831062}\n",
      "baseBag1 :\n",
      "\tridge :\n",
      "\t{'avg_r2': -5.0001846336694866, 'best_alpha': 0.0001}\n",
      "\tbayesian :\n",
      "\t{'avg_r2': -5.03298791629721}\n",
      "\tols :\n",
      "\t{'avg_r2': -5.0001845384645387}\n",
      "baseBag2 :\n",
      "\tridge :\n",
      "\t{'avg_r2': 0.38041165527712323, 'best_alpha': 0.0001}\n",
      "\tbayesian :\n",
      "\t{'avg_r2': 0.38250302463294616}\n",
      "\tols :\n",
      "\t{'avg_r2': 0.38468857282490077}\n",
      "baseBag3 :\n",
      "\tridge :\n",
      "\t{'avg_r2': 0.38426476878961346, 'best_alpha': 0.0001}\n",
      "\tbayesian :\n",
      "\t{'avg_r2': 0.38455324447250128}\n",
      "\tols :\n",
      "\t{'avg_r2': 0.38487183289691923}\n",
      "baseBag4 :\n",
      "\tridge :\n",
      "\t{'avg_r2': 0.37929689869875544, 'best_alpha': 0.0001}\n",
      "\tbayesian :\n",
      "\t{'avg_r2': 0.38014102228110819}\n",
      "\tols :\n",
      "\t{'avg_r2': 0.38376325988336785}\n",
      "baseBag5 :\n",
      "\tridge :\n",
      "\t{'avg_r2': 0.36680605675897454, 'best_alpha': 0.0001}\n",
      "\tbayesian :\n",
      "\t{'avg_r2': 0.36619159785746197}\n",
      "\tols :\n",
      "\t{'avg_r2': 0.3717657364088845}\n",
      "baseBag6 :\n",
      "\tridge :\n",
      "\t{'avg_r2': 0.37031674178269136, 'best_alpha': 0.0001}\n",
      "\tbayesian :\n",
      "\t{'avg_r2': 0.36752073431374532}\n",
      "\tols :\n",
      "\t{'avg_r2': 0.37642315070801374}\n"
     ]
    }
   ],
   "source": [
    "for bag in sorted(random_feature_bags.keys()):\n",
    "    print(bag + ' :')\n",
    "    for model in res[bag]:\n",
    "        print(\"\\t\" + str(model) + ' :')\n",
    "        print(\"\\t\" + str(res[bag][model]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key, modelDict in res.items():\n",
    "    best_model = \"ols\"\n",
    "    best_r2 = modelDict[best_model][\"avg_r2\"]\n",
    "    if modelDict[\"ridge\"][\"avg_r2\"] > best_r2:\n",
    "        best_model = \"ridge\"\n",
    "        best_r2 = modelDict[\"ridge\"][\"avg_r2\"]\n",
    "    if modelDict[\"bayesian\"][\"avg_r2\"] > best_r2:\n",
    "        best_model = \"bayesian\"\n",
    "        best_r2 = modelDict[\"bayesian\"][\"avg_r2\"]\n",
    "    res[key][\"best_model\"] = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r2_to_bags = {}\n",
    "for bag, modelDict in res.items():\n",
    "    bag_model = modelDict[\"best_model\"]\n",
    "    bag_r2 = modelDict[bag_model][\"avg_r2\"]\n",
    "    r2_to_bags[bag_r2] = bag\n",
    "\n",
    "best_bags = []\n",
    "for r2 in sorted(r2_to_bags.keys(), reverse = True):\n",
    "    best_bags.append(r2_to_bags[r2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bag17',\n",
       " 'bag6',\n",
       " 'bag12',\n",
       " 'bag18',\n",
       " 'bag10',\n",
       " 'bag19',\n",
       " 'bag0',\n",
       " 'bag5',\n",
       " 'bag1',\n",
       " 'bag11',\n",
       " 'bag7',\n",
       " 'bag3',\n",
       " 'bag15',\n",
       " 'bag14',\n",
       " 'bag16',\n",
       " 'baseBag0',\n",
       " 'baseBag3',\n",
       " 'baseBag2',\n",
       " 'baseBag4',\n",
       " 'baseBag6',\n",
       " 'baseBag5',\n",
       " 'bag2',\n",
       " 'bag8',\n",
       " 'bag4',\n",
       " 'bag13',\n",
       " 'bag9',\n",
       " 'baseBag1']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag17 :\n",
      "features = ['sMacd_Hist' 'match' 'pSignal' 'pSma_60' 'pSma_10' 'nomatch' 'pEma_3'\n",
      " 'pEma_w' 'pEma_30' 'pEma_15']\n",
      "r2 = 0.742498195216\n",
      "model = ridge\n",
      "alpha = 0.0001\n",
      "bag6 :\n",
      "features = ['sEma_w' 'sMacd' 'match' 'pEma_90' 'sSma_15' 'sEma_30' 'pEma_3' 'sEma_10'\n",
      " 'pSignal' 'sEma_60']\n",
      "r2 = 0.715620620965\n",
      "model = ridge\n",
      "alpha = 0.001\n",
      "bag12 :\n",
      "features = ['sSma_90' 'pSma_7' 'pEma_3' 'sEma_7' 'pK' 'sEma_3' 'pSignal' 'sBuzz'\n",
      " 'match' 'sSma_30']\n",
      "r2 = 0.714417426289\n",
      "model = ridge\n",
      "alpha = 0.001\n",
      "bag18 :\n",
      "features = ['pD' 'pSma_10' 'pEma_3' 'pEma_10' 'sEma_30' 'sentiment_minus_1'\n",
      " 'p_minus_3' 'pEma_90' 'pMacd_Hist' 'p_minus_2']\n",
      "r2 = 0.699764348777\n",
      "model = ridge\n",
      "alpha = 0.0001\n",
      "bag10 :\n",
      "features = ['p_minus_4' 'sD' 'pSma_15' 'pEma_7' 'pMacd_Hist' 'sentiment' 'sSma_w'\n",
      " 'pEma_w' 'sEma_w' 'sentiment_minus_1']\n",
      "r2 = 0.693431283576\n",
      "model = bayesian\n",
      "bag19 :\n",
      "features = ['pSma_15' 'pSignal' 'p_minus_3' 'p_minus_4' 'nomatch' 'p_minus_2'\n",
      " 'sSma_60' 'pEma_3' 'sSma_90' 'p']\n",
      "r2 = 0.685410979198\n",
      "model = ridge\n",
      "alpha = 0.0001\n",
      "bag0 :\n",
      "features = ['pEma_90' 'sentiment_minus_3' 'sLow' 'p_minus_1' 'pEma_3' 'sSma_60'\n",
      " 'sEma_3' 'sSma_30' 'pK' 'pSma_60']\n",
      "r2 = 0.676770182216\n",
      "model = ridge\n",
      "alpha = 0.0001\n",
      "bag5 :\n",
      "features = ['pSma_90' 'sEma_30' 'sEma_15' 'pEma_7' 'sEma_60' 'sSma_7' 'sEma_7'\n",
      " 'sEma_10' 'sD' 'pEma_3']\n",
      "r2 = 0.671870701036\n",
      "model = ridge\n",
      "alpha = 0.0001\n",
      "bag1 :\n",
      "features = ['nomatch' 'pSma_7' 'sSma_w' 'pD' 'sEma_w' 'pEma_15' 'p_minus_3' 'pEma_3'\n",
      " 'p' 'sHigh']\n",
      "r2 = 0.661192375823\n",
      "model = bayesian\n",
      "bag11 :\n",
      "features = ['pEma_7' 'p' 'sSma_60' 'pMacd' 'sSma_7' 'sSma_10' 'sentiment_minus_3'\n",
      " 'sEma_7' 'pSma_w' 'pSma_60']\n",
      "r2 = 0.634685573257\n",
      "model = ridge\n",
      "alpha = 0.001\n"
     ]
    }
   ],
   "source": [
    "for bag in best_bags[:10]:\n",
    "    print(bag + ' :')\n",
    "    print('features = ' + str(random_feature_bags[bag]))\n",
    "    print('r2 = ' + str(res[bag][res[bag][\"best_model\"]][\"avg_r2\"]))\n",
    "    print('model = ' + str(res[bag][\"best_model\"]))\n",
    "    if res[bag][\"best_model\"] == \"ridge\":\n",
    "        print('alpha = ' + str(res[bag][\"ridge\"][\"best_alpha\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
